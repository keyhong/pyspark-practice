{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3373c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "101b809e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/28 07:29:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=RDD_practice>\n"
     ]
    }
   ],
   "source": [
    "# pyspark 연결하기 (SparkSession이 아닌, SparkContext로 연결하는 고전적인 방법임.)\n",
    "conf = SparkConf().setAppName(\"RDD_practice\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b50f27",
   "metadata": {},
   "source": [
    "# Part1: RDD 생성 및 기본 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a65d0a",
   "metadata": {},
   "source": [
    "# RDD를 생성하는 두 가지 방법:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777829d",
   "metadata": {},
   "source": [
    "1. 드라이버 프로그램에서 기존의 컬렉션을 Parallelizing 하는 방법\n",
    "2. 공유 파일 시스템, HDFS, HBase, Hadoop InputFormat을 제공하는 데이터 소스 등과 같은 외부 데이터 저장소를 참조하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86462044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 37, 38, 3, 34, 17, 5, 39, 27, 33]\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 데이터 생성\n",
    "import random\n",
    "random_list = random.sample(range(0, 40), 10)\n",
    "print(random_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6628fba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8, 37, 38, 3, 34, 17, 5, 39, 27, 33]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD 생성\n",
    "rdd1 = sc.parallelize(random_list, 4)\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bc1ad78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Distribute a local Python collection to form an RDD. Using xrange\n",
       "is recommended if the input represents a range for performance.\n",
       "\n",
       ">>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
       "[[0], [2], [3], [4], [6]]\n",
       ">>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
       "[[], [0], [], [2], [4]]\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.9/dist-packages/pyspark/context.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.parallelize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f032b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 37], [38, 3], [34, 17], [5, 39, 27, 33]]\n",
      "The two partitions [[8, 37], [38, 3]]\n",
      "The last partition [5, 39, 27, 33]\n"
     ]
    }
   ],
   "source": [
    "# 전체 파티션 수 조회\n",
    "print(rdd1.getNumPartitions())\n",
    "\n",
    "# 파티션별 분산된 데이터 조회\n",
    "print(rdd1.glom().collect())\n",
    "\n",
    "# 파티션별 분산된 데이터 중 2개의 파티션만 보기\n",
    "print(\"The two partitions\", rdd1.glom().take(2))\n",
    "\n",
    "# 마지막 파티션 출력\n",
    "print(\"The last partition\", rdd1.glom().collect()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1bc42dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count() : RDD 전체 element 개수\n",
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df81bca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first() : 첫 번쨰 파티션의 첫 번째 element 값\n",
    "rdd1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e3ea1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 : [39]\n",
      "Top 2 : [39, 38]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# top(): 값이 높은 상위 N개의 elemenet 출력\n",
    "# drive 프로세스에 모든 element를 보낸 후, 정렬(sort) 후 값을 반환하므로 데이터의 사이즈가 크면 사용에 주의가 필요하다.\n",
    "print(\"Top 1 :\", rdd1.top(1))\n",
    "print(\"Top 2 :\", rdd1.top(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbaea9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8, 37, 17, 5, 33, 38, 34, 3, 39, 27]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct() : 중복을 제거한 element 반환\n",
    "rdd1.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ceb1e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def function map : [[27, 114], [117, 12], [105, 54], [18, 120, 84, 102]]\n",
      "lambda function map : [[27, 114], [117, 12], [105, 54], [18, 120, 84, 102]]\n"
     ]
    }
   ],
   "source": [
    "def myfunc(item):\n",
    "    return (item+1) * 3\n",
    "\n",
    "# map(): 각 elemnt에 function을 적용한 결과를 RDD로 반환 (파티션의 수는 변하지 않는다.)\n",
    "rdd_map = rdd1.map(myfunc)\n",
    "print(\"def function map :\", rdd_map.glom().collect())\n",
    "\n",
    "# map의 인자로 lambda(익명함수)도 사용 가능하다.\n",
    "rdd_map = rdd1.map(lambda item: (item+1) * 3)\n",
    "print(\"lambda function map :\", rdd_map.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5690d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [3], [], [39, 27, 33]]\n",
      "After filtering count : 4\n",
      "Repartition : [[3, 39, 27, 33]]\n"
     ]
    }
   ],
   "source": [
    "# filter(): element를 조건(function)에 부합하는 element만 필터링하여 RDD 반환 (파티션의 수는 변하지 않는다. 따라서, empty한 파티션이 발생 가능하다.)\n",
    "# empty한 파티션은 GC(Garbage Collect) 기술인 repartition/coalese릍 통해 차후 제거 가능하다.\n",
    "rdd_filter = rdd1.filter(lambda x: x%3==0)\n",
    "print(rdd_filter.glom().collect())\n",
    "\n",
    "print(\"After filtering count :\", rdd_filter.count())\n",
    "print(\"Repartition :\", rdd_filter.repartition(1).glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b1588d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatMap Collect:  [10, 13, 39, 42, 40, 43, 5, 8, 36, 39, 19, 22, 7, 10, 41, 44, 29, 32, 35, 38]\n",
      "map Collect:  [[10, 13], [39, 42], [40, 43], [5, 8], [36, 39], [19, 22], [7, 10], [41, 44], [29, 32], [35, 38]] \n",
      "\n",
      "flatMap glom collect [[10, 13, 39, 42], [40, 43, 5, 8], [36, 39, 19, 22], [7, 10, 41, 44, 29, 32, 35, 38]]\n",
      "map glom collect:  [[[10, 13], [39, 42]], [[40, 43], [5, 8]], [[36, 39], [19, 22]], [[7, 10], [41, 44], [29, 32], [35, 38]]] \n",
      "\n",
      "flatMap Partition Num: 4\n",
      "map Partition Num: 4\n"
     ]
    }
   ],
   "source": [
    "# flatMap(): 파티션별 모든 element를 하나의 단일 컬렉션으로 반환한다. (파티션 수가 변하는 것이 아니다. 파티션 내부의 차원을 한 단계 flat하게 만든다.)\n",
    "rdd_flatmap = rdd1.flatMap(lambda x: [x+2, x+5])\n",
    "\n",
    "# map()과 비교하기\n",
    "rdd_map = rdd1.map(lambda x: [x+2, x+5])\n",
    "\n",
    "# 비교 시작\n",
    "print(\"flatMap Collect: \", rdd_flatmap.collect())\n",
    "print(\"map Collect: \", rdd_map.collect(), \"\\n\")\n",
    "\n",
    "print(\"flatMap glom collect\", rdd_flatmap.glom().collect())\n",
    "print(\"map glom collect: \", rdd_map.glom().collect(), \"\\n\")\n",
    "\n",
    "print(\"flatMap Partition Num:\", rdd_flatmap.getNumPartitions())\n",
    "print(\"map Partition Num:\", rdd_map.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c9082ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce(): 각 파티션별 배열(1차원)의 element에 대해 function을 수행한다. 파티션별 function의 return 값에 대해 파티션 전체에 대한 reduce를 계속 실행한다.\n",
    "# reduce()는 transformation이 아닌, action 함수이다.\n",
    "\n",
    "# flatMap glom collect: [[9, 12, 39, 42], [23, 26, 24, 27], [8, 11, 19, 22], [17, 20, 30, 33, 40, 43, 32, 35]] 라면,\n",
    "# step1 : [((9+12)+39)+42], [((23+26)+24)+27], [((8+11)+19)+22], [((((((17+20)+30)+33)+40)+43)+32)+35]]\n",
    "# step2 : (([102] + [100]) + [60]) + [250]\n",
    "# Return : 512\n",
    "\n",
    "rdd_flatmap.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76677c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 3, 24.1, 13.74, 241]\n"
     ]
    }
   ],
   "source": [
    "# 기술 통계 (Descriptive statistic)\n",
    "print([rdd1.max(), rdd1.min(), rdd1.mean(), round(rdd1.stdev(), 2), rdd1.sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "477e58b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1 glom collect: [[8, 37], [38, 3], [34, 17], [5, 39, 27, 33]]\n",
      "rdd1 mapPartition: [45, 41, 51, 104]\n",
      "rdd1 glom Partition: [[45], [41], [51], [104]]\n"
     ]
    }
   ],
   "source": [
    "# mapPartitions(): 파티션별 function을 하여 특정 결과를 반환한다. action에 reduce가 있다면, transformation에는 mapPartition이라고 생각하면 쉽다.\n",
    "# 각 파티션의 element에 대해 fuct을 적용한 중간 결과(yield)를 다음 element의 func 연산을 할 떄 사용하므로, 제너레이터에 사용하는 yield를 사용해야 한다!\n",
    "\n",
    "def myfunc(partition):\n",
    "    sumatition = 0\n",
    "    \n",
    "    for item in partition:\n",
    "        sumatition = sumatition + item\n",
    "        \n",
    "    yield sumatition\n",
    "\n",
    "print(\"rdd1 glom collect:\", rdd1.glom().collect())\n",
    "print(\"rdd1 mapPartition:\", rdd1.mapPartitions(myfunc).collect())\n",
    "print(\"rdd1 glom Partition:\", rdd1.mapPartitions(myfunc).glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b674655",
   "metadata": {},
   "source": [
    "# Part2: Advanced RDD Transformations and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb66afd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD 1: [8, 37, 38, 3, 34, 17, 5, 39, 27, 33]\n",
      "RDD 2: [1, 14, 37, 20, 28, 10, 13, 3]\n",
      "RDD union: [8, 37, 38, 3, 34, 17, 5, 39, 27, 33, 1, 14, 37, 20, 28, 10, 13, 3]\n",
      "RDD Partition Num: 6\n"
     ]
    }
   ],
   "source": [
    "# union(): 두 RDD를 결합하여 새로운 RDD를 반환한다. 결합한 RDD의 파티션 수는 결합에 사용된 RDD의 파티션의 총 합과 같다.\n",
    "\n",
    "print(\"RDD 1:\", rdd1.collect())\n",
    "\n",
    "rdd2 = sc.parallelize([1, 14, 37, 20, 28, 10, 13, 3], 2)\n",
    "print(\"RDD 2:\", rdd2.collect())\n",
    "\n",
    "rdd_union = rdd1.union(rdd2)\n",
    "print(\"RDD union:\", rdd_union.collect())\n",
    "\n",
    "print(\"RDD Partition Num:\", rdd_union.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b33ca81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD intersection: [37, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD intersection glom collect: [[], [37], [], [3], [], []]\n",
      "RDD Partition Num: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# intersection(): 두 RDD의 공통된 element만 추출하여 새로운 RDD를 반환한다. 교차시킨 RDD의 파티션 수는 교차에 사용된 RDD의 파티션의 총 합과 같다.\n",
    "rdd_intersection = rdd1.intersection(rdd2)\n",
    "\n",
    "print(\"RDD intersection:\", rdd_intersection.collect())\n",
    "print(\"RDD intersection glom collect:\" , rdd_intersection.glom().collect())\n",
    "print(\"RDD Partition Num:\", rdd_intersection.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edfdf55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find empty partitions\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for item in rdd_intersection.glom().collect():\n",
    "    if len(item) == 0:\n",
    "        counter += 1\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18b7326e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[37, 3]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coalesce(numPartitions): 파티션의 수를 줄인다. \n",
    "rdd_intersection.coalesce(1).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac53237b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37, 3, 5, 27, 38]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takeSample(withReplacement, num, [seed]): 무작위로 데이터를 추출하여 반환한다.\n",
    "# collect()와 마찬가지로, parallel 하지 않고 drive 메모리에 모든 element를 모아 랜덤 추출하기 때문에 스파크 클러스터가 손상 될 수 있어 사용에 주의가 필요하다.\n",
    "rdd1.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "739015a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 8, 17, 27]\n",
      "[39, 38, 37, 34, 33]\n"
     ]
    }
   ],
   "source": [
    "# takeOrdered(n, [ordering]): drive 메모리에 모든 element를 모아 오름차순으로 인자 수 만큼의 element 반환\n",
    "print(rdd1.takeOrdered(5))\n",
    "\n",
    "# 내림차순 정렬\n",
    "print(rdd1.takeOrdered(5, key=lambda x: -x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c41519da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce():\n",
    "rdd1.reduce(lambda x,y: x-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c055b979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1, 4), (7, 10), (5, 7)], [(1, 12), (7, 12), (9, 1), (7, 4)]]\n",
      "[(1, 16), (7, 26), (5, 7), (9, 1)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key  value\n",
       "0    1      4\n",
       "1    7     10\n",
       "2    5      7\n",
       "3    1     12\n",
       "4    7     12\n",
       "5    9      1\n",
       "6    7      4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey():\n",
    "rdd_Rbk = sc.parallelize([(1, 4), (7, 10), (5, 7), (1, 12), (7, 12), (9, 1), (7, 4)], 2)\n",
    "print(rdd_Rbk.glom().collect())\n",
    "print(rdd_Rbk.reduceByKey(lambda x,y: x+y).collect())\n",
    "\n",
    "\n",
    "# user friendly visualization\n",
    "import pandas as pd\n",
    "Counter = pd.DataFrame({\"key\": rdd_Rbk.keys().collect(), \"value\": rdd_Rbk.values().collect()})\n",
    "Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8d7055f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 16), (5, 7), (7, 26), (9, 1)]\n",
      "[(9, 1), (7, 26), (5, 7), (1, 16)]\n"
     ]
    }
   ],
   "source": [
    "# sortByKey():\n",
    "print(rdd_Rbk.reduceByKey(lambda x,y: x+y).sortByKey().collect())\n",
    "\n",
    "# reverse sortByKey()\n",
    "print(rdd_Rbk.reduceByKey(lambda x,y: x+y).sortByKey(False).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69f40399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 2, 7: 3, 5: 1, 9: 1})\n",
      "dict_items([(1, 2), (7, 3), (5, 1), (9, 1)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# countByKey()\n",
    "print(rdd_Rbk.countByKey())\n",
    "\n",
    "print(rdd_Rbk.countByKey().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26822b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPartitions: 2\n",
      "1 [4, 12]\n",
      "7 [10, 12, 4]\n",
      "5 [7]\n",
      "9 [1]\n"
     ]
    }
   ],
   "source": [
    "# groupByKey(): 대규모의 shuffle이 발생 할 수 있기 때문에, filter를 먼저 사용 후 사용을 권장\n",
    "rdd_group = rdd_Rbk.groupByKey()\n",
    "print(\"NumPartitions:\", rdd_group.getNumPartitions())\n",
    "\n",
    "for item in rdd_group.collect():\n",
    "    print(item[0], [values for values in item[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "755dd5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 12, 4]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lookup(key): \n",
    "rdd_Rbk.lookup(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28471045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[58] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache():\n",
    "# 기본적으로, 각 transformed된 RDD는 action을 실행 할 때마다 다시 계산(compute) 될 수 있다.₩\n",
    "# 하지만, persist (또는 cache) 메서드를 사용하여 RDD를 메모리에 유지 할 수도 있다.\n",
    "# 이런 경우에 Spark가 클러스터 전체에 elements를 유지하여 다음 쿼리 때 훨씬 더 빠르게 접근 할 수 있다.\n",
    "\n",
    "# 메모리에서 결과를 계산하면 스파크는 Garbage Collector를 사용하여 LRU 알고리즘에 의해 메모리가 부족시 객체를 삭제하게 된다.\n",
    "# 이런 경우가 있기 떄문에, cache를 통해 읽기 내용을 캐시 내부에 저장 할 수 있다. 캐시된 RDD는 가비지 컬렉터에 의해 삭제되지 않는다.\n",
    "\n",
    "# cache()는 메모리에 캐시\n",
    "rdd_Rbk.cache()\n",
    "\n",
    "# persist()는 저장하는 수준을 사용자가 지정 할 수 있다. default로는 MEMORY_ONLY를 사용하고 있다.\n",
    "# https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence\n",
    "# 옵션에 따라 자바 RDD 객체를 직렬화 하여 JVM에 저장하고, 부족시 디스크에 저장하여 필요시 읽게 할 수도 있다.\n",
    "rdd_Rbk.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4901d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mrdd_Rbk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Persist this RDD with the default storage level (`MEMORY_ONLY`).\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.9/dist-packages/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_Rbk.cache?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfe21105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mrdd_Rbk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorageLevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStorageLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Set this RDD's storage level to persist its values across operations\n",
       "after the first time it is computed. This can only be used to assign\n",
       "a new storage level if the RDD does not have a storage level set yet.\n",
       "If no storage level is specified defaults to (`MEMORY_ONLY`).\n",
       "\n",
       ">>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
       ">>> rdd.persist().is_cached\n",
       "True\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.9/dist-packages/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_Rbk.persist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8b16a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[58] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache 해제\n",
    "rdd_Rbk.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9243545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[58] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# MEMORY_AND_DISK 레벨로 캐싱\n",
    "rdd_Rbk.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "68d3b10d79c6e02d0d1f7bb5595fc6acdd9cd892dc842c813ff1c5403a22a80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
